{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:51.388218Z",
     "start_time": "2025-03-27T06:03:51.259078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "id": "8cbba7a777653be5",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:51.403807Z",
     "start_time": "2025-03-27T06:03:51.392219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "LEARNING_RATE = 0.0005\n",
    "z_dim=10\n",
    "activation_func='leaky_relu'\n",
    "init='Normal'\n",
    "EPOCHS =250\n",
    "num_clusters=7"
   ],
   "id": "542f2e3075402f7b",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:51.526205Z",
     "start_time": "2025-03-27T06:03:51.511203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "########################## 학습 시드 고정\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "##########################\n"
   ],
   "id": "4bbf3bc562c7ee4d",
   "outputs": [],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:52.042732Z",
     "start_time": "2025-03-27T06:03:51.644757Z"
    }
   },
   "source": [
    "##dataset load##\n",
    "data_type='sim'# #real_data 는 type에 따라 'real_merged','merged' 'real_sensitive' 'real_resistant' 총 세가지 있고, simulation data 이름은 데이터는sim_ALT_c5.tsv, 타겟은 sim_clades_c7.tsv\n",
    "num_feature=300\n",
    "alt = pd.read_csv(f'data/{data_type}_ALT_{num_feature}.tsv', sep='\\t', header=None)\n",
    "\n",
    "alt=alt.fillna(-1)\n",
    "alt_transposed = alt.T\n",
    "alt_data = alt_transposed.to_numpy()\n",
    "alt_data[(alt_data != 0) & (alt_data != -1)] = 1\n",
    "\n",
    "\n",
    "##### data_type이 simulation 데이터일 때만 clade있음 \n",
    "if 'sim' in data_type:\n",
    "    clade = pd.read_csv(f'data/{data_type}_clades_c{num_clusters}.tsv', sep='\\t', header=None)\n",
    "    clade_transposed = clade.T\n",
    "    clade_data= clade_transposed.to_numpy()\n"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:52.174403Z",
     "start_time": "2025-03-27T06:03:52.161390Z"
    }
   },
   "cell_type": "code",
   "source": "clade_data.shape",
   "id": "f7ce11cf4026a722",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:52.308154Z",
     "start_time": "2025-03-27T06:03:52.295151Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets=alt_data\n",
    "train_loader = DataLoader(datasets, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(datasets, batch_size=len(datasets), shuffle=False, pin_memory=True)"
   ],
   "id": "f35faf2409b29a81",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:52.426290Z",
     "start_time": "2025-03-27T06:03:52.411288Z"
    }
   },
   "cell_type": "code",
   "source": "input_dim=datasets[0].shape[0]",
   "id": "55e8e5b33d1e7a94",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:52.559703Z",
     "start_time": "2025-03-27T06:03:52.544701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, x_dim, hidden_dim_1, hidden_dim_2,latent_dim, init_type='Normal',\n",
    "                 activation_type='ELU'):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        if activation_type == 'ELU':\n",
    "            self.activation_fn = nn.ELU()\n",
    "        elif activation_type == 'leaky_relu':\n",
    "            self.activation_fn = nn.LeakyReLU()\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation type. Choose 'relu' or 'leaky_relu'.\")\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            nn.Linear(x_dim, hidden_dim_1),\n",
    "            nn.BatchNorm1d(hidden_dim_1),\n",
    "            self.activation_fn,\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dim_1, hidden_dim_2),\n",
    "            nn.BatchNorm1d(hidden_dim_2),\n",
    "            self.activation_fn,\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dim_2, latent_dim)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.ModuleList([\n",
    "            nn.Linear(latent_dim, hidden_dim_2),\n",
    "            nn.BatchNorm1d(hidden_dim_2),\n",
    "            self.activation_fn,\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dim_2, hidden_dim_1),\n",
    "            nn.BatchNorm1d(hidden_dim_1),\n",
    "            self.activation_fn,\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(hidden_dim_1, x_dim),\n",
    "            self.activation_fn\n",
    "        ])\n",
    "        \n",
    "        initialize_weights(self, init_type=init_type)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_outputs = []\n",
    "        # Encoder: Apply layers and save activation function outputs\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "            if layer==self.activation_fn:  # Activation is applied after each Linear \n",
    "                encoder_outputs.append(x)\n",
    "\n",
    "        latent = x\n",
    "        \n",
    "        decoder_outputs = []\n",
    "        # Decoder: Apply layers and save activation function outputs\n",
    "        for layer in self.decoder:\n",
    "            x = layer(x)\n",
    "            if layer==self.activation_fn:  # Activation is applied after each Linear layer\n",
    "                decoder_outputs.append(x)\n",
    "\n",
    "        return latent, x,encoder_outputs, decoder_outputs\n"
   ],
   "id": "6e1dba77d5d4fc66",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:52.717492Z",
     "start_time": "2025-03-27T06:03:52.704489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def initialize_weights(net, init_type='Xavier'):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "            if init_type == 'He':\n",
    "                # He 초기화는 Kaiming 초기화의 특별한 경우로, 일반적으로 'leaky_relu'를 비선형성으로 사용\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='leaky_relu')\n",
    "            elif init_type == 'Xavieru':\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif init_type == 'Xavier':\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            elif init_type == 'Normal':\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "            \n",
    "            elif init_type == 'Kaiming_Uniform':\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported initialization type: {init_type}\")\n",
    "            # 편향 (bias) 초기화\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)"
   ],
   "id": "bfd91fa833128cd2",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:52.826321Z",
     "start_time": "2025-03-27T06:03:52.811320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoEncoder(input_dim, 200,   100,  z_dim,init_type=init,activation_type=activation_func)\n",
    "\n",
    "model=model.to(device)"
   ],
   "id": "f8c0d988d2b2e5e2",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:03:52.943154Z",
     "start_time": "2025-03-27T06:03:52.928037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion_reconst = nn.MSELoss(reduction='none') \n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8)"
   ],
   "id": "3e957ee1b06f82e",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-27T06:03:53.079036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "loss_list = []\n",
    "model.train()\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    train_loss = 0\n",
    "    for batch_idx, x in enumerate(train_loader):\n",
    "        x = x.float()\n",
    "        x   = x.to(device)\n",
    "        mask = (x!= -1).float()\n",
    "        optimizer.zero_grad()\n",
    "        latent, outputs,encoder_outputs, decoder_outputs = model(x)\n",
    "        x_hat = outputs\n",
    "        loss = criterion_reconst(x_hat, x)\n",
    "        loss = loss *mask\n",
    "        loss = loss.sum() / mask.sum()\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss_train = train_loss / len(train_loader)\n",
    "    loss_list.append(avg_loss_train)"
   ],
   "id": "f7f7ca6ec1ba7a3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5e64718b9264f48b1c818a8d1cbd51b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[68], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(EPOCHS)):\n\u001B[0;32m      4\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, x \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m      6\u001B[0m         x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m      7\u001B[0m         x   \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PFL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    632\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    633\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 634\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    635\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    636\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    637\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    638\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PFL\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    676\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    677\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 678\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    679\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    680\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PFL\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PFL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001B[0m, in \u001B[0;36mdefault_collate\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m    203\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault_collate\u001B[39m(batch):\n\u001B[0;32m    204\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001B[39;00m\n\u001B[0;32m    206\u001B[0m \u001B[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    262\u001B[0m \u001B[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001B[39;00m\n\u001B[0;32m    263\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 264\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdefault_collate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PFL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m collate_fn_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    118\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m elem_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[1;32m--> 119\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mcollate_fn_map\u001B[49m\u001B[43m[\u001B[49m\u001B[43melem_type\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcollate_fn_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcollate_fn_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m collate_type \u001B[38;5;129;01min\u001B[39;00m collate_fn_map:\n\u001B[0;32m    122\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, collate_type):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PFL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001B[0m, in \u001B[0;36mcollate_numpy_array_fn\u001B[1;34m(batch, collate_fn_map)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np_str_obj_array_pattern\u001B[38;5;241m.\u001B[39msearch(elem\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mstr) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem\u001B[38;5;241m.\u001B[39mdtype))\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m collate([torch\u001B[38;5;241m.\u001B[39mas_tensor(b) \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\PFL\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    168\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np_str_obj_array_pattern\u001B[38;5;241m.\u001B[39msearch(elem\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mstr) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    169\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(default_collate_err_msg_format\u001B[38;5;241m.\u001B[39mformat(elem\u001B[38;5;241m.\u001B[39mdtype))\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m collate([\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m b \u001B[38;5;129;01min\u001B[39;00m batch], collate_fn_map\u001B[38;5;241m=\u001B[39mcollate_fn_map)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:18:39.158543Z",
     "start_time": "2025-03-27T06:18:39.127716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch_idx, x in enumerate(train_loader):\n",
    "        print(len(x))"
   ],
   "id": "a6796274ae60beff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "64\n",
      "8\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T03:35:40.722504Z",
     "start_time": "2025-03-27T03:35:40.664492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "for batch_idx, x in enumerate(test_loader):\n",
    "    x = x.float()\n",
    "    x = x.to(device)\n",
    "    mask = (x != -1).float()\n",
    "    latent, x_hat, encoder_outputs, decoder_outputs = model(x)\n",
    "    loss = criterion_reconst(x_hat, x)\n",
    "    ## missing value 는 loss 계산에서 무시\n",
    "    loss = loss * mask\n",
    "\n",
    "    loss = loss.sum() / mask.sum()\n",
    "    ##\n",
    "    test_loss += loss.item()\n",
    "    z_v = latent\n",
    "    z_v = z_v.cpu().detach().numpy()\n",
    "    latent_feature = np.vstack((z_v))\n",
    "\n",
    "avg_loss_test = test_loss / len(test_loader)\n",
    "\n"
   ],
   "id": "263ad3bdbe48e02e",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T03:35:40.827528Z",
     "start_time": "2025-03-27T03:35:40.814525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Latent feature 및 인덱스 배열 준비\n",
    "X = latent_feature\n",
    "\n",
    "decoder_output_0 = decoder_outputs[0].cpu().detach().numpy()\n",
    "decoder_output_1 = decoder_outputs[1].cpu().detach().numpy()\n",
    "\n",
    "# Latent feature와 decoder outputs 결합\n",
    "combined_features = np.concatenate((X, decoder_output_0), axis=1)"
   ],
   "id": "e58c530d17d2eacc",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T06:43:34.807256Z",
     "start_time": "2025-03-27T06:43:34.797158Z"
    }
   },
   "cell_type": "code",
   "source": "combined_features.shape",
   "id": "baf9dcc3dca7ca75",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 110)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T03:35:51.089717Z",
     "start_time": "2025-03-27T03:35:40.919359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "# Bayesian Gaussian Mixture Clustering\n",
    "bgmm = BayesianGaussianMixture(n_components=num_clusters, max_iter=500, n_init=10, random_state=RANDOM_SEED)\n",
    "bgmm.fit(combined_features)\n",
    "y_pred = bgmm.predict(combined_features)\n",
    "np.savetxt(f\"outputs/merge_zd1.tsv\", y_pred, delimiter=\"\\t\", fmt=\"%d\")\n"
   ],
   "id": "c80cf6f744d90f4e",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T03:35:51.224753Z",
     "start_time": "2025-03-27T03:35:51.210750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## simulation data라면 target 이 있어서 ARI score 점수 계산 수행\n",
    "if 'sim' in data_type:\n",
    "    from sklearn.metrics import adjusted_rand_score\n",
    "    target=clade_data\n",
    "    ari_score = adjusted_rand_score(target[0], y_pred)\n",
    "    print(\"ARI Score for simulation dataset:\", ari_score)\n"
   ],
   "id": "71908b3ee86ebb28",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARI Score for simulation dataset: 0.9683295997040139\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T03:35:51.360042Z",
     "start_time": "2025-03-27T03:35:51.345776Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9eb129ee3a7b6c74",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
